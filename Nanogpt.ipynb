{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1ec472",
   "metadata": {
    "id": "1a1ec472"
   },
   "source": [
    "# Lecture 16: NanoGPT Implementation with W&B Tracking\n",
    "\n",
    "Extended from [karpathy/nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) and made even more nano.\n",
    "\n",
    "Modified to remove functions related to loading the pre-trained GPT2 model. If\n",
    "you would like to use the pre-trained model, please refer to the original\n",
    "repository.\n",
    "We also remove flash attention and any optimizations, so things are simpler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf88f9c",
   "metadata": {
    "id": "2cf88f9c",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db2ed05",
   "metadata": {
    "id": "0db2ed05"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619a7b5",
   "metadata": {
    "id": "9619a7b5"
   },
   "source": [
    "Here, we will also use `einops` for more readable tensor manipulations.\n",
    "The syntax can be thought of as a more readable way to write `permute`, `reshape`, `transpose`, etc.,\n",
    "via _describing the indices_ before and after the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0912fc80",
   "metadata": {
    "id": "0912fc80"
   },
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93f2cd",
   "metadata": {
    "id": "fb93f2cd"
   },
   "source": [
    "For example, let's flatten a tensor, and then restore it to its original shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9185bd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9185bd2",
    "outputId": "7c37edbd-3df4-43fc-e788-ef488902cb7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 2, 3, 4) # 32 elements in batch, 2 channels, height 3, width 4\n",
    "y = rearrange(x, 'batch channel height width -> batch (height width) channel')\n",
    "print(y.shape)\n",
    "z = rearrange(y, 'batch (height width) channel -> batch channel height width', height=3, width=4)\n",
    "assert torch.allclose(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1497b1",
   "metadata": {
    "id": "bc1497b1"
   },
   "source": [
    "We can also use `reduce` to sum over some dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd34df7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fd34df7",
    "outputId": "92c01ea2-9556-4783-8dd5-0cc128a591d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 2, 3, 4)\n",
    "y = reduce(x, 'batch channel height width -> batch channel', 'sum')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7572b",
   "metadata": {
    "id": "94c7572b"
   },
   "source": [
    "In this notebook, we want to follow best practices for deep learning, and track our experiments.\n",
    "\n",
    "As a demo, we will work with a small Shakespeare text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4324f295",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4324f295",
    "outputId": "dbbfb789-5b17-45bd-f281-080993873b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size (characters): 57\n",
      "Final vocabulary size: 500\n",
      "['\\n', ' ', '!', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'e ', 'th', 't ', 'ou', 's ', ' th', ', ', 'd ', 'en', 'er', 'in', 'an', 'y ', 'or', 'll', 'you', 'on', ' the ', 'it', ':\\n', 'ar', 'ha', '\\n\\n', 'ir', 'st ', ',\\n', 'es', 'o ', 'ea', ' s', 'iti', 'at', 'ic', 'EN', 'itiz', 'itizen', 'no', 'Th', 'Citizen', 'Citizen:\\n', ' m', '.\\n\\n', 'is ', ' w', ' a', 'ell', 'hat ', ' you', 'irst ', 'of', ' the', 'e th', 'ing', 're', 'us', 'First ', 'First Citizen:\\n', 'and ', 'om', ' c', 've', 'is', 'to', 'al', 'IU', 'IUS', 'IUS:\\n', '. ', 'our', ' b', 'for', 'to ', 'st', ' h', 'el', 'un', 'at ', '?\\n\\n', 'the ', 'ith', 'it ', 'wh', 'ed ', 've ', 'gh', 'MEN', 'MENEN', 'MENENIUS:\\n', 'ke ', 'ur', 'ra', 'ec', 'oo', 'oul', 'The ', 'ear', 'us ', 'em', 't, ', 'im', 'wa', '.\\n\\nFirst Citizen:\\n', ' p', '\\nW', 'ould ', 'li', 'il', 'e, ', \"'s \", 's, ', 'ri', '\\nA', 'ce', 'are ', 'll ', 'ol', 'ow', 'ey ', 'and', 'ed', 'bell', '--', 'ther', 'ver', 'y, ', 'wer', 'not ', 'es ', 'one ', 'ma', 's\\n', 'now', '; ', 'ac', 'ven', 'rea', 'id', ' d', 'sa', 'ich', 'ti', 'What ', \"' the \", 'bl', 'nd ', 'e s', 'pea', 'goo', 'ain', 'od', 'end', 'ough', ' wh', ' your', 'ans', 'ru', ' their', ', th', 'ro', 'peak', 'di', 'know', 'ch', 'pe', 'pl', 'have ', ', the ', 'ut ', 'up', 'e\\n', 'e m', 'ff', ' f', 'he ', 'ous', \"o' the \", 'e that ', ' they ', 'They ', '.\\n\\nMENENIUS:\\n', ' ma', 'ves ', 'answer', 'e w', 'you ', 'to the ', 'et ', 'a ', 'good ', 'oun', 'po', ': ', 'gain', 'all', 'tr', '?\\n\\nFirst Citizen:\\n', 'ly', 'was ', 'mo', 'ter', 'ong', 'Wh', 'your', 'For', 'That ', ',\\nA', 'e,\\n', 'hat', 'All', 'All:\\n', 'You', 'res', 'am', 'ius ', \"e'\", 'king', 'be ', 'Sec', 'Secon', 'Second ', 'Second Citizen:\\n', 'coun', 'ici', 'but ', ';\\n', 'ink', 'ts ', ' them', 'with', ' this ', 'gainst ', 'ten', 'sel', ' in', 'ust ', 'other', 'my ', 's,\\n', 'e the ', 's of', ' st', 'ut', ',\\nAnd ', 'ang', 'hear', 'an ', 'our ', 'own', 'one', 'poor', 'igh', 'the', 'o d', 'ul', 'und', 'ce ', 'es, ', 'ome ', 'for ', 'him', 'com', 'well', 'did ', ', you', '? ', 'ity', 'ap', 'wor', ', I', 'str', 'strong', 'fri', 'friend', 'most ', 'ant', 'e, wh', 'ill ', 'Of', 'ever', 'like ', '! ', ' g', 'rain', 't\\n', '\\nT', \"'ll\", 'bod', 'ber', 'belly ', \"'d\", '?\\n\\nMENENIUS:\\n', 'rom', 'rece', 'recei', 'say', 'MA', 'MAR', 'MARC', 'MARCIUS:\\n', 'speak', 'olve', 'f ', 'We ', 'we ', ', and ', 'orn', ' at ', '!\\n\\n', 'pat', '.\\nW', 'fe', 'if', 'whol', 'ess ', ' the l', 'as ', 'par', 'part', ' ab', 'su', ' the g', 'ung', 'er ', 'ge', '.\\n\\nSecond Citizen:\\n', 'against ', ' his ', 'and c', 'con', 'ive ', 'por', ' with', 'rou', 'what ', 'say ', 'se ', 'his ', 'ly ', '; wh', 'hel', 'I ', ' of', 'cus', 'ion', 'pet', 'sh', 'ere', 'ere ', 'atter', 'sen', 'hall ', ' you ', 'sir', 'tell', 'tell you', '\\nThe ', ' your ', ',\\nThe ', 'make ', 's you', 'ind', 'ail', 'ale', 'ut, ', 'erves ', 'body', 'mem', 'member', \"'d \", 'belly', 'la', 'tion', ' com', '--\\n\\n', 'loo', 'What', 'then', ' the c', ', that ', '\\nB', '\\nAnd ', 'as', ' their ', 'pro', 'fam', 'ish', '?\\n\\nAll:\\n', 'olved', 'irst', 'know ', 'Mar', 'Marc', \"'t\", 'ill', ' him', '\\nI', \"'t \", 'ore ', ' on', 'd, ', 'ted ', 'patr', 'patrici', 'patrician', 'ity ', ' would ', 'would ', 'fl', 'they', 'aff', 'ob', 't of', ' our', 'vent', 'eir', ': h', \"e's \", 'do', ' to the ', 'mon', 'Con', ' what ', 'he ha', 'countr', 'conten', 't to ', 'give ', 'eing', 'roud', 'I s', 'to you', ', what ', ' that ', ' though', 'sc', 'men', 'to\\n', 'even', 'can', 'help', ' must ', 'in ', 'no ']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Fetch and preprocess Shakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text[:10000]  # Using first 10,000 characters\n",
    "\n",
    "# Start with character-level tokenization\n",
    "tokens = list(text) # A list of all the characters in the text\n",
    "\n",
    "# Create initial vocabulary (unique characters)\n",
    "vocab = sorted(list(set(tokens)))\n",
    "print(f\"Initial vocab size (characters): {len(vocab)}\") # All the vocab in the tokens\n",
    "\n",
    "max_vocab_size = 500\n",
    "\n",
    "# Perform merges\n",
    "while len(vocab) < max_vocab_size:\n",
    "    # Count frequencies of adjacent pairs\n",
    "    pairs = collections.Counter()\n",
    "    for j in range(len(tokens) - 1):\n",
    "        pair = (tokens[j], tokens[j+1])\n",
    "        pairs[pair] += 1\n",
    "\n",
    "    # If no pairs are left, we're done\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    # Find most frequent pair\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    best_pair_str = best_pair[0] + best_pair[1]\n",
    "\n",
    "    # Add the merged pair to vocabulary\n",
    "    vocab.append(best_pair_str)\n",
    "\n",
    "    # Replace all occurrences of the pair in the token list\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == best_pair[0] and tokens[i+1] == best_pair[1]:\n",
    "            new_tokens.append(best_pair_str)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    tokens = new_tokens\n",
    "\n",
    "# Create mappings between tokens and IDs\n",
    "token_to_id = {token: i for i, token in enumerate(vocab)}\n",
    "id_to_token = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "# Encode the text using our BPE vocabulary\n",
    "encoded = [token_to_id[token] for token in tokens]\n",
    "data = torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Final vocabulary size: {vocab_size}\")\n",
    "\n",
    "# This gives us:\n",
    "# data, vocab_size, token_to_id, id_to_token\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Ibw24591J4uu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ibw24591J4uu",
    "outputId": "e91c63a4-03cd-4acb-ab34-14d649d97a95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([113,  11,  35,  ...,  97,  35, 248])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our data is just categorical now:\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "WLUcJ0YsG4xC",
   "metadata": {
    "id": "WLUcJ0YsG4xC"
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, block_size=16):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data) - self.block_size)\n",
    "    def __getitem__(self, idx):\n",
    "        # We give the data in fixed chunks:\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        return chunk[:-1], chunk[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276fa31",
   "metadata": {
    "id": "c276fa31"
   },
   "source": [
    "First, we create a layer norm with an optional bias parameter (can be toggled off):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15a0f80",
   "metadata": {
    "id": "d15a0f80"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9711a",
   "metadata": {
    "id": "43c9711a"
   },
   "source": [
    "We create a *causal* self-attention layer since\n",
    "GPT-2 is a decoder-only model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c78a4ce",
   "metadata": {
    "id": "9c78a4ce"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "            1, 1, config.block_size, config.block_size\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h=self.n_head)  # (B, nh, T, hs)\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h=self.n_head)  # (B, nh, T, hs)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h=self.n_head)  # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = rearrange(y, 'b h t d -> b t (h d)')  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22650baa",
   "metadata": {
    "id": "22650baa"
   },
   "source": [
    "After each self-attention, we apply a 1-layer MLP for the feedforward network.\n",
    "\n",
    "This also includes dropout (though by default it is off in GPT-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "021baed8",
   "metadata": {
    "id": "021baed8"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34e8e1",
   "metadata": {
    "id": "3c34e8e1"
   },
   "source": [
    "A single GPT-2 block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9376a846",
   "metadata": {
    "id": "9376a846"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee23a0",
   "metadata": {
    "id": "22ee23a0"
   },
   "source": [
    "n.b., `@dataclass` is a good way to define a configuration in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "764dce7e",
   "metadata": {
    "id": "764dce7e"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # Adjusted for faster training\n",
    "    block_size: int = 16          # [1024]\n",
    "    vocab_size: int = len(vocab)  # [50304]\n",
    "    n_layer: int = 3      # [12]\n",
    "    n_head: int = 2       # [12]\n",
    "    n_embd: int = 32      # [768]\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13867a48",
   "metadata": {
    "id": "13867a48"
   },
   "source": [
    "Finally, GPT itself is a stack of these blocks, with extra\n",
    "parameters for the input embedding and the final output linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "906e4cd0",
   "metadata": {
    "id": "906e4cd0"
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),  # Token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),  # Position embeddings\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Scale by input dimension\n",
    "            std = 1.0 / math.sqrt(module.in_features)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Scale by embedding dimension\n",
    "            std = 1.0 / math.sqrt(module.embedding_dim)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(rearrange(logits, 'b t v -> (b t) v'), rearrange(targets, 'b t -> (b t)'), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b210b",
   "metadata": {
    "id": "414b210b"
   },
   "source": [
    "## Training + W&B Demo\n",
    "\n",
    "Let's test our model with a training loop and track progress with Weights & Biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeefe396",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "aeefe396",
    "outputId": "ca04a7cd-c1b0-4b42-c017-0eea76c038b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 14, with 53,376 parameters\n",
      "num non-decayed parameter tensors: 26, with 1,312 parameters\n",
      "Epoch 0, Loss: 6.060898780822754\n",
      "Epoch 1, Loss: 5.786173343658447\n",
      "Epoch 2, Loss: 5.566069602966309\n",
      "Epoch 3, Loss: 5.246857643127441\n",
      "Epoch 4, Loss: 4.920835971832275\n",
      "Epoch 5, Loss: 4.6106672286987305\n",
      "Epoch 6, Loss: 4.3157958984375\n",
      "Epoch 7, Loss: 4.040560722351074\n",
      "Epoch 8, Loss: 3.759960174560547\n",
      "Epoch 9, Loss: 3.4803078174591064\n",
      "Epoch 10, Loss: 3.1892292499542236\n",
      "Epoch 11, Loss: 2.908733606338501\n",
      "Epoch 12, Loss: 2.6430392265319824\n",
      "Epoch 13, Loss: 2.4047434329986572\n",
      "Epoch 14, Loss: 2.1739206314086914\n",
      "Epoch 15, Loss: 1.9398540258407593\n",
      "Epoch 16, Loss: 1.7195838689804077\n",
      "Epoch 17, Loss: 1.5091955661773682\n",
      "Epoch 18, Loss: 1.3294602632522583\n",
      "Epoch 19, Loss: 1.1861246824264526\n",
      "Epoch 20, Loss: 1.0832997560501099\n",
      "Epoch 21, Loss: 1.0123296976089478\n",
      "Epoch 22, Loss: 0.9456520080566406\n",
      "Epoch 23, Loss: 0.8823789358139038\n",
      "Epoch 24, Loss: 0.8141838312149048\n",
      "Epoch 25, Loss: 0.7832832932472229\n",
      "Epoch 26, Loss: 0.7219309210777283\n",
      "Epoch 27, Loss: 0.7392908334732056\n",
      "Epoch 28, Loss: 0.6809978485107422\n",
      "Epoch 29, Loss: 0.6762995719909668\n"
     ]
    }
   ],
   "source": [
    "# Config and model (smaller for speed)\n",
    "config = GPTConfig()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Configure optimizer with higher learning rate for faster demo\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Data and loader\n",
    "dataset = TextDataset(data, config.block_size)\n",
    "loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# W&B initialization for experiment tracking\n",
    "wandb.init(project=\"nanoGPT-cambridge\", config=config.__dict__)\n",
    "\n",
    "# Track the entirety of the model parameters:\n",
    "wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "# Training loop with W&B logging\n",
    "for epoch in range(30):\n",
    "    for inputs, targets in loader:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss = model(inputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log loss to W&B\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Generate a sample text after each epoch\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    gen = model.generate(idx, max_new_tokens=10)[0].tolist()\n",
    "    gen_text = ''.join(id_to_token[i] for i in gen)\n",
    "    wandb.log({\"generated\": gen_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "YDlZUA0VNJNx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YDlZUA0VNJNx",
    "outputId": "ed0b11f0-6e27-4cdc-b313-dcedddadd551"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nailke ppledicke m mas\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da32ae",
   "metadata": {
    "id": "11da32ae"
   },
   "source": [
    "## Challenge & Wrap-Up\n",
    "\n",
    "### Challenge\n",
    "\n",
    "Try tweaking `n_head` or `learning_rate` and log the results to W&B. Can you improve the loss?\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bab36",
   "metadata": {
    "id": "090bab36"
   },
   "outputs": [],
   "source": [
    "# Experiment with different configurations\n",
    "config = GPTConfig(n_head=4)  # Try more attention heads\n",
    "# OR\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=5e-3, betas=(0.9, 0.95), device_type='cpu')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
