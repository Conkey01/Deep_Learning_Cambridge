{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95af2a33",
   "metadata": {
    "id": "95af2a33"
   },
   "source": [
    "# PyTorch II: Neural Network Training (MLP vs CNN on MNIST-1D)\n",
    "\n",
    "In this notebook, we will train neural networks in PyTorch, comparing a **Multi-Layer Perceptron (MLP)** and a **Convolutional Neural Network (CNN)** on the **MNIST-1D** dataset.\n",
    "MNIST-1D is a synthetic 1-dimensional analogue of MNIST, designed to be low-compute while highlighting model differences.\n",
    "According to the original MNIST-1D result, a linear model achieves ~32% accuracy, an MLP around 68%, and a CNN about 94%.\n",
    "\n",
    "What we'll cover:\n",
    "\n",
    "- Setting up a PyTorch training loop with data loading, loss computation, backpropagation, and optimization\n",
    "- Differences between MLPs and CNNs in architecture and performance\n",
    "- Best practices for training and monitoring progress\n",
    "- Awareness of high-level libraries like PyTorch Lightning and Hugging Face Accelerate\n",
    "\n",
    "Let's get started by installing dependencies and preparing the dataset.\n",
    "\n",
    "\n",
    "## Setup: Installing Dependencies and Loading Data\n",
    "\n",
    "First, we'll install the `mnist1d` package and import PyTorch, NumPy, and Matplotlib for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665f0bc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "665f0bc7",
    "outputId": "8f238332-9ed5-4edc-9abc-f87241c39c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mnist1d\n",
      "  Downloading mnist1d-0.0.2.post1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting requests (from mnist1d)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting numpy (from mnist1d)\n",
      "  Downloading numpy-2.2.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting matplotlib (from mnist1d)\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from mnist1d)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->mnist1d)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->mnist1d)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->mnist1d)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->mnist1d)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mcranmer/Cambridge Courses/2024-2025/m2_2025/m2_dl_for_dis/.venv/lib/python3.12/site-packages (from matplotlib->mnist1d) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib->mnist1d)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->mnist1d)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mcranmer/Cambridge Courses/2024-2025/m2_2025/m2_dl_for_dis/.venv/lib/python3.12/site-packages (from matplotlib->mnist1d) (2.9.0.post0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mnist1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75804b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a75804b4",
    "lines_to_next_cell": 0,
    "outputId": "0fb9fe24-9d65-4d2a-e765-78b0d38fc6be"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available, and use it for faster training if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706475c8",
   "metadata": {
    "id": "706475c8"
   },
   "source": [
    "Now, let's load the MNIST-1D dataset.\n",
    "The dataset consists of 4000 training and 1000 test examples, each being a 1D sequence of length 40 representing a synthetic \"digit\" (0-9).\n",
    "The label is the digit class (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4827a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cf4827a",
    "outputId": "64da65d7-4668-43b6-c00d-f0b35419476e"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST-1D dataset\n",
    "from urllib.request import urlopen\n",
    "import pickle\n",
    "\n",
    "url = 'https://github.com/greydanus/mnist1d/raw/master/mnist1d_data.pkl'\n",
    "data = pickle.load(urlopen(url))\n",
    "\n",
    "data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QZvu34uN4GbK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZvu34uN4GbK",
    "outputId": "943ff323-2247-41d9-e921-55fd0e615d8f"
   },
   "outputs": [],
   "source": [
    "# Generate the default MNIST-1D dataset\n",
    "X_train, y_train, X_test, y_test = data['x'], data['y'], data['x_test'], data['y_test']\n",
    "\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "# Let's peek at the first training sample and label\n",
    "print(\"First training sample (x[0]):\", X_train[0])\n",
    "print(\"First training label (y[0]):\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115eb6a8",
   "metadata": {
    "id": "115eb6a8"
   },
   "source": [
    "Each data sample is a 40-dimensional vector representing a digit's \"pen strokes\" in 1D.\n",
    "Let's visualize a couple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81b817",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "3d81b817",
    "outputId": "bf143c18-c7f3-430c-c9ac-94af2042a04c"
   },
   "outputs": [],
   "source": [
    "# Plot a couple of sample 1D signals from the dataset\n",
    "plt.figure(figsize=(6,4))\n",
    "for digit in [0, 1]:  # let's visualize the first two classes for example\n",
    "    idx = np.where(y_train == digit)[0][0]  # find first index of this digit in training set\n",
    "    plt.plot(X_train[idx], label=f\"Class {digit}\")\n",
    "plt.title(\"Example MNIST-1D signals for two classes\")\n",
    "plt.xlabel(\"Position (1D)\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a209f88",
   "metadata": {
    "id": "1a209f88"
   },
   "source": [
    "## MLP vs CNN: Intuition and Architecture Differences\n",
    "\n",
    "Recall: MLP is a fully-connected neural network where every neuron connects to all neurons in the next layer. For our 40-dimensional input, each neuron looks at the entire input at once. MLPs don't explicitly leverage spatial structure in the data - they must learn relevant patterns from scratch, which can make them prone to overfitting if not regularized.\n",
    "\n",
    "Recall: CNNs **sparse connectivity** and **weight sharing** in the form of kernels which slide across the input. Each filter has fewer weights and is reused across different input positions. This makes CNNs parameter-efficient and naturally able to detect local patterns regardless of position (translation invariance). This inductive bias works well for structured data like images or sequential signals.\n",
    "\n",
    "Now, let's define two models for our task:\n",
    "\n",
    "1. **A simple MLP** with one hidden layer.\n",
    "2. **A simple 1D CNN** with one or two convolutional layers.\n",
    "\n",
    "We'll then compare their performance on the MNIST-1D data.\n",
    "\n",
    "## Define the Neural Network Models\n",
    "\n",
    "We'll define the model architectures using PyTorch's `nn.Module`.\n",
    "For the MLP, we'll use a single hidden layer with ReLU activation.\n",
    "For the CNN, we'll use a small network with convolutional layers and pooling.\n",
    "Both models will output 10 logits (one for each digit class 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60980a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e60980a",
    "lines_to_next_cell": 0,
    "outputId": "9acea643-6a8b-4f80-e3b3-23b9fc2cb9b2"
   },
   "outputs": [],
   "source": [
    "# Define a simple MLP with one hidden layer\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=40, hidden_size=64, num_classes=10):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, input_size)\n",
    "        out = self.hidden(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "# Define a simple 1D CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 1 input channel (since our signals are 1D sequences), let's use 8 filters for first conv\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, padding=0)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)  # will halve the sequence length\n",
    "        # Second conv: take 8 channels from conv1, output 16 filters\n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=0)\n",
    "        # After conv and pooling layers, we'll flatten and use a fully connected layer\n",
    "        # Compute the output dimension after conv+pool layers:\n",
    "        # Input length 40 -> conv1(kernel5 no padding) -> length 36 -> pool -> length 18\n",
    "        # 18 -> conv2(kernel3) -> length 16 -> pool -> length 8\n",
    "        # So final feature map has size 16 (channels) x 8 (length) = 128 features\n",
    "        self.fc = nn.Linear(16 * 8, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, 1, sequence_length)\n",
    "        out = self.conv1(x)         # (batch, 8, 36)\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)        # (batch, 8, 18)\n",
    "        out = self.conv2(out)       # (batch, 16, 16)\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)        # (batch, 16, 8)\n",
    "        out = out.view(out.size(0), -1)  # flatten to (batch, 16*8)\n",
    "        out = self.fc(out)          # (batch, num_classes)\n",
    "        return out\n",
    "\n",
    "# Instantiate the models\n",
    "mlp_model = SimpleMLP().to(device)\n",
    "cnn_model = SimpleCNN().to(device)\n",
    "\n",
    "print(mlp_model)\n",
    "print(cnn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FyRNX6-845Iv",
   "metadata": {
    "id": "FyRNX6-845Iv"
   },
   "source": [
    "**Explanation:** The MLP has one hidden layer (40->64) with ReLU and an output layer (64->10). The CNN uses two 1D conv layers - conv1 (8 filters, size 5) and conv2 (16 filters, size 3), each followed by ReLU and max pooling. A final fully connected layer produces 10 outputs. Though small, the CNN's architecture gives it advantages.\n",
    "\n",
    "Let's compare their parameter counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HAUEuWa7465Q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAUEuWa7465Q",
    "outputId": "4a3d841e-cb00-4333-ee2c-b8511dda7db9"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"MLP model parameters: {count_parameters(mlp_model)}\")\n",
    "print(f\"CNN model parameters: {count_parameters(cnn_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a70c2",
   "metadata": {
    "id": "090a70c2"
   },
   "source": [
    "## Preparing Data Loaders\n",
    "\n",
    "We'll convert our NumPy arrays into PyTorch `Dataset` and `DataLoader` objects for batch loading during training, using `TensorDataset` to wrap our tensors.\n",
    "\n",
    "For evaluation, we'll use the provided test set each epoch, though in practice you might want a separate validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k7s3bAdD4-02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7s3bAdD4-02",
    "outputId": "6b26ad11-a0f6-4ba3-8ea2-9ccdd79d708a"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and test\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Batches in train loader:\", len(train_loader))\n",
    "print(\"Batches in test loader:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fMzvqgYq5At7",
   "metadata": {
    "id": "fMzvqgYq5At7"
   },
   "source": [
    "We set a batch size of 64.\n",
    "Shuffling is enabled for training data (best practice to shuffle training examples each epoch for stochastic gradient descent).\n",
    "\n",
    "## Training the Models\n",
    "\n",
    "We will train both models using the Cross-Entropy Loss and Adam optimizer, recording loss and accuracy metrics during training.\n",
    "\n",
    "A typical PyTorch training loop involves:\n",
    "\n",
    "1. Set model to training mode (`model.train()`)\n",
    "2. For each batch:\n",
    "   - Forward pass and compute loss\n",
    "   - Backpropagate and update weights\n",
    "   - Zero gradients (**important!!**)\n",
    "3. Evaluate on test set with `model.eval()`\n",
    "\n",
    "We'll track both loss and accuracy to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9a809ec",
   "metadata": {
    "id": "e9a809ec"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=10, learning_rate=0.001):\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Lists to store metrics per epoch\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()  # put model in training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs if not isinstance(model, SimpleCNN) else inputs.unsqueeze(1))\n",
    "            # (Note: for CNN, inputs shape [batch, 40] needs reshaping to [batch, 1, 40])\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)  # accumulate summed loss (item() gives scalar)\n",
    "            # Compute number of correct predictions for this batch\n",
    "            _, predicted = torch.max(outputs, 1)  # get index of max logit\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # Compute average training loss and accuracy for the epoch\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Evaluation on test data\n",
    "        model.eval()  # evaluation mode\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Disable gradient calculation for efficiency\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs if not isinstance(model, SimpleCNN) else inputs.unsqueeze(1))\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        test_loss = test_loss / total\n",
    "        test_acc = correct / total\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch}/{epochs} -> \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% || \"\n",
    "                f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2VV03aOR5Rhs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VV03aOR5Rhs",
    "outputId": "0c04ccdf-8d71-48af-aadc-78d7a899c9b1"
   },
   "outputs": [],
   "source": [
    "print(\"Training MLP...\")\n",
    "# Train the MLP model\n",
    "mlp_train_losses, mlp_train_acc, mlp_test_losses, mlp_test_acc = train_model(mlp_model, train_loader, test_loader, epochs=20, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6cfa0e",
   "metadata": {
    "id": "da6cfa0e",
    "lines_to_next_cell": 0
   },
   "source": [
    "We train for 20 epochs (this should be sufficient to see clear trends).\n",
    "The training loop prints the loss and accuracy for both training and test sets at each epoch.\n",
    "Monitoring both is a good practice to detect overfitting: if training accuracy keeps increasing but test accuracy starts decreasing, the model might be overfitting to the training data.\n",
    "\n",
    "After training the MLP, let's train the CNN with the same number of epochs for a fair comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7301a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc7301a5",
    "outputId": "f4341971-0505-413f-a4f3-21e822b67e13"
   },
   "outputs": [],
   "source": [
    "# Train the CNN model\n",
    "print(\"\\nTraining CNN...\")\n",
    "cnn_train_losses, cnn_train_acc, cnn_test_losses, cnn_test_acc = train_model(cnn_model, train_loader, test_loader, epochs=20, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39616a48",
   "metadata": {
    "id": "39616a48"
   },
   "source": [
    "## Visualizing Training Progress\n",
    "\n",
    "Now that both models are trained, let's visualize the loss and accuracy curves over epochs for the MLP and CNN.\n",
    "This will help us compare their learning speed and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0033bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "fe0033bf",
    "outputId": "ffa858d4-5463-448b-def0-a3aed8dd35cf"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, mlp_train_losses, 'b-o', label='MLP Train Loss')\n",
    "plt.plot(epochs, mlp_test_losses, 'b--s', label='MLP Test Loss')\n",
    "plt.plot(epochs, cnn_train_losses, 'r-o', label='CNN Train Loss')\n",
    "plt.plot(epochs, cnn_test_losses, 'r--s', label='CNN Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('MLP vs CNN Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy curves\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, [a*100 for a in mlp_train_acc], 'b-o', label='MLP Train Acc')\n",
    "plt.plot(epochs, [a*100 for a in mlp_test_acc], 'b--s', label='MLP Test Acc')\n",
    "plt.plot(epochs, [a*100 for a in cnn_train_acc], 'r-o', label='CNN Train Acc')\n",
    "plt.plot(epochs, [a*100 for a in cnn_test_acc], 'r--s', label='CNN Test Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('MLP vs CNN Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b07968",
   "metadata": {
    "id": "f6b07968"
   },
   "source": [
    "**Discussion:** Examine the plots to see:\n",
    "- **Convergence**: Which model converges faster and reaches a lower loss? CNNs typically do better on structured data by efficiently extracting features.\n",
    "- **Accuracy**: The CNN should achieve higher test accuracy than the MLP, which tends to level off lower.\n",
    "- **Overfitting**: Look for gaps between training and test curves. The MLP may overfit more due to having more parameters, while the CNN's local structure helps it generalize better.\n",
    "\n",
    "## Inspecting Learned Weights for Intuition\n",
    "\n",
    "Let's inspect the learned weights of the first layer of each model to understand what patterns they detect:\n",
    "- For the **MLP**, we'll examine weight vectors (length 40) connecting inputs to the 64 hidden units\n",
    "- For the **CNN**, we'll look at the 8 convolutional filters (length 5) that detect local features\n",
    "\n",
    "**MLP first-layer weight vectors:** Each hidden neuron may specialize to detect patterns in different parts of the input. Let's visualize some weight vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201beb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "1201beb7",
    "lines_to_next_cell": 0,
    "outputId": "c76665d7-a965-461f-b08f-53c39fc31a91"
   },
   "outputs": [],
   "source": [
    "# Get the weight matrix of the MLP hidden layer (shape: [hidden_size, input_size])\n",
    "mlp_weights = mlp_model.hidden.weight.detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "for i in range(5):  # plot first 5 hidden unit weight vectors\n",
    "    plt.plot(mlp_weights[i], label=f'Neuron {i}')\n",
    "plt.title(\"MLP Hidden Layer Weight Vectors (first 5 neurons)\")\n",
    "plt.xlabel(\"Input index (0-39)\")\n",
    "plt.ylabel(\"Weight value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa7870",
   "metadata": {
    "id": "88aa7870"
   },
   "source": [
    "Each filter's 5 values determine what pattern it detects in any 5-position segment of the input. For example, a filter `[0.5, 1.0, -0.2, -0.8, -0.5]` detects where signals rise then fall.\n",
    "\n",
    "Common filter patterns include:\n",
    "- `[+, +, +, +, +]` (all positive): detects regions of high values\n",
    "- `[+, 0, -, 0, +]` (alternating): detects up-down-up patterns  \n",
    "- Large middle coefficient: detects peaks at center\n",
    "\n",
    "CNN filters are easier to interpret since they look at local patterns. MLP weights mix information globally, making them harder to understand directly.\n",
    "\n",
    "**Summary:**\n",
    "- MLPs must use the entire input for each neuron, discovering local patterns on their own if needed\n",
    "- CNN filters explicitly show what local features they detect (edges, peaks, slopes) in each 5-value window\n",
    "\n",
    "## Conclusion and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1c497",
   "metadata": {
    "id": "09b1c497"
   },
   "source": [
    "For general best practices, please read the following blogpost: https://karpathy.github.io/2019/04/25/recipe/\n",
    "\n",
    "**Note on advanced tools:** Tools like **PyTorch Lightning** and **Hugging Face Accelerate** can simplify training by handling boilerplate code for loops, devices, and distributed training. While these tools speed up development, understanding the manual training process remains valuable.\n",
    "\n",
    "Some ways to experiment further:\n",
    "\n",
    "- Modify the MLP architecture (layers, neurons)\n",
    "- Tune hyperparameters (learning rate, batch size, etc.)\n",
    "- Adjust CNN parameters (filter sizes, number)\n",
    "- Visualize model predictions on test data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
